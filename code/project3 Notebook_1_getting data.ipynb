{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [1. Problem Statement](#Problem_Statement)\n",
    "- [2. Importing Libraries](#Importing_Libraries)\n",
    "- [3. Getting posts from the subreddits](#Getting_posts_from_the_two_subreddits) \n",
    "\n",
    "\n",
    " ........ Continued in project3 Notebook_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The challenge is to study the contents of a new post on Reddit and classify as which of 2 subreddits the post should be tagged to. \n",
    "\n",
    "### The subreddits being considered are 'r/vegetarian' and 'r/lowimpactlifestyle'. \n",
    "\n",
    "### Low Impact Lifestyle - An active group on Reddit working towards a sustainable future by sharing of ideas and information to promote and enjoy a â€˜Low Impact Lifestyle.\n",
    "\n",
    "### The objective is to develop a Machine Learning model that can correctly identify posts highlighting a low impact lifestyle, i.e., one that aims at creating less waste and use of less resources. Although people who are  following a vegetarian diet may be part of the movement towards a low impact lifestyle, others  may be vegetarian for various other reasons and not subscribe to this particular lifestyle as a form of earth sustainability. It is thus possible that when they post on the Low Impact Lifestyle reddit, they may have done so in error and the post needs to be redirected to subreddit 'Vegetarian'.  This redirection will also reduce confusion in minds of members on both subreddits on the objective of their respective page. \n",
    "\n",
    "### A classification model will be developed from the existing posts from the two subreddits. The model will be tested on new posts on the subreddits to check its efficacy.\n",
    "\n",
    "### The details are covered in 2 Jupyter notebooks\n",
    "### - Notebook 1 - covering collection of the posts from the subreddits and saving them as .csv files\n",
    "### - Notebook 2 - retrieving info stored in the .csv files to prepare the model after necessary cleaning and pre-processing. This will be followed for modelling, evaluation and selection of model ending with testing of random sample posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting posts from the two subreddits - Vegetarian and Low Impact Lifestyle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The posts on the subreddits are collected by accessing the url and doing webscraping to collect content using json. A  cover user-agent name is used to mask the accessing of the json file by python. In addition, a random sleep generating code is included to allow the collection to continue undetected by the Reddit system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls for the subreddits\n",
    "\n",
    "# url_1 = 'https://www.reddit.com/r/vegetarian/.json'\n",
    "# url_1 = 'https://www.reddit.com/r/vegetarian/new.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to get subreddits\n",
    "\n",
    "def get_posts_reddit(url, times):\n",
    "    \n",
    "    after = None\n",
    "\n",
    "    for a in range(times):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        \n",
    "    # cover user-agent used to mask access of json file by python     \n",
    "        res = requests.get(current_url, headers={'User-agent': 'Sloppy Joe 2.0'})\n",
    "    \n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "    \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        \n",
    "        \n",
    "    # add post to directory\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        \n",
    "    # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,60)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Collection of posts from subreddit 'Vegetarian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of posts from subreddit 'vegetarian'\n",
    "\n",
    "posts = []\n",
    "get_posts_reddit('https://www.reddit.com/r/vegetarian/.json', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking number of posts collected\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of posts to a dataframe (df) for easy handling \n",
    "# saving a .csv copy as backup \n",
    "# to enable working on further data exploration \n",
    "\n",
    "veg_posts=pd.DataFrame(posts)\n",
    "veg_posts.to_csv('../datasets/vegetarian.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining shape of df\n",
    "veg_posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Collection of posts from subreddit 'Low Impact Lifestyle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of posts from subreddit 'low impact lifestlye'\n",
    "\n",
    "posts = []\n",
    "get_posts_reddit('https://www.reddit.com/r/lowimpactlifestyle/.json', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_posts=pd.DataFrame(posts)\n",
    "lil_posts.to_csv('../datasets/lowimpactlifestyle.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
